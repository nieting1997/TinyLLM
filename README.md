# TinyLLM  
&gt; 参考自：《从零构建大模型》

## 实现效果
- 通过 PyTorch 从零实现一个类 GPT 架构的大语言模型，完成文本生成、分类任务微调等功能。

## 技术细节

### 1. 数据处理与分词

#### 文本分词 (Tokenization)
- 使用 **Byte Pair Encoding (BPE)** 算法处理文本  
- 将词元转换为词元 ID，引入特殊上下文词元  
- 使用滑动窗口进行数据采样  

#### 词元嵌入 (Token Embedding)
- 创建词元嵌入，编码单词位置信息  

### 2. 注意力机制实现

#### 自注意力机制 (Self-Attention)
- 计算所有输入词元的注意力权重  
- 实现带可训练权重的自注意力机制  

#### 因果注意力 (Causal Attention)
- 利用因果注意力隐藏未来词汇   

#### 多头注意力 (Multi-Head Attention)
- 将单头注意力扩展到多头注意力  
- 通过权重划分实现多头注意力  

### 3. Transformer 块实现

#### 前馈网络 (FeedForward)
- 两层线性变换 + GELU 激活  

#### 层归一化 (Layer Normalization)
- 使用层归一化进行归一化激活  

#### Transformer 块
- 残差连接 
- 连接 Transformer 块中的注意力层和线性层  

### 4. 完整 GPT 模型实现
- 构建一个大语言模型架构  
- 实现 GPT 模型  

### 5. 训练与优化

#### 文本生成策略
- 使用 **温度缩放** 控制随机性  
- **Top-k 采样** 和 **Top-p 采样** 策略  

#### 训练循环  

### 6. 微调技术

#### 有监督微调 (SFT)
- 添加分类头  
- 计算分类损失和准确率  

#### 指令微调 (Instruction Tuning)
- 通过微调遵循人类指令  
- 安全与指令对齐：Instruction Tuning 与有害输出的控制手段  

### 7. 模型评估与应用

#### 评估指标
- 计算训练集和验证集的损失  
- 使用大语言模型作为垃圾消息分类器  

#### 文本生成
- 生成文本  
- 修改文本生成函数以支持不同解码策略
